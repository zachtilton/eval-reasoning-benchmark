Appendix G.1: API Configuration and Batch Processing

Model API Endpoints and Authentication
python
# API Configuration Dictionary
API_CONFIG = {
    "gpt_5": {
        "endpoint": "https://api.openai.com/v1/chat/completions",
        "model_id": "gpt-5.2",
        "auth_type": "bearer",
        "api_key_env": "OPENAI_API_KEY"
    },
    "claude_opus_4_5": {
        "endpoint": "https://api.anthropic.com/v1/messages",
        "model_id": "claude-opus-4-5-20251101",
        "auth_type": "x-api-key",
        "api_key_env": "ANTHROPIC_API_KEY"
    },
    "gemini_3_pro": {
        "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro:generateContent",
        "model_id": "gemini-3-pro",
        "auth_type": "api_key_param",
        "api_key_env": "GOOGLE_API_KEY"
    },
    "deepseek_v3": {
        "endpoint": "https://api.deepseek.com/v1/chat/completions",
        "model_id": "deepseek-chat",
        "auth_type": "bearer",
        "api_key_env": "DEEPSEEK_API_KEY"
    },
    "kimi_k2": {
        "endpoint": "https://api.moonshot.cn/v1/chat/completions",
        "model_id": "moonshot-v1-32k",
        "auth_type": "bearer",
        "api_key_env": "MOONSHOT_API_KEY"
    },
    "glm_4_7": {
        "endpoint": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
        "model_id": "glm-4-plus",
        "auth_type": "bearer",
        "api_key_env": "ZHIPU_API_KEY"
    }
}
Standardized API Parameters
python
# Universal parameters across all models
STANDARD_PARAMS = {
    "temperature": 0,           # Maximum determinism
    "max_tokens": 500,          # Output limit
    "top_p": 1.0,              # No nucleus sampling
    "frequency_penalty": 0,     # No repetition penalty
    "presence_penalty": 0       # No topic diversity penalty
}

# Expected token usage (for cost estimation and validation)
EXPECTED_TOKENS = {
    "input_avg": 750,           # Prompt + fragment
    "output_avg": 250,          # Classification + rationale
    "input_max": 1200,          # Upper bound for validation
    "output_max": 500           # API limit
}
Fragment Execution Order Randomization
python
import random
import pandas as pd

def generate_execution_order(fragment_ids, seed=42):
    """
    Generate randomized execution order distinct from gold standard sequence.
    
    Args:
        fragment_ids: List of fragment IDs (e.g., ['F_001', 'F_002', ...])
        seed: Random seed for reproducibility
    
    Returns:
        List of fragment IDs in randomized order
    """
    random.seed(seed)
    execution_order = fragment_ids.copy()
    random.shuffle(execution_order)
    
    # Log randomization
    order_df = pd.DataFrame({
        'execution_position': range(1, len(execution_order) + 1),
        'fragment_id': execution_order
    })
    order_df.to_csv('execution_order_log.csv', index=False)
    
    return execution_order
Batch Processing Workflow
python
import time
from datetime import datetime

def process_fragments_batch(fragments, models, prompts, runs_per_combo=3, 
                            batch_size=1000, rate_limit_delay=5):
    """
    Process fragments sequentially with all model-prompt combinations.
    
    Args:
        fragments: List of fragment dictionaries with 'id' and 'text'
        models: List of model identifiers from API_CONFIG
        prompts: List of prompt types ['zero_shot', 'few_shot']
        runs_per_combo: Number of independent runs per fragment-model-prompt
        batch_size: API calls per session (500-1000 recommended)
        rate_limit_delay: Seconds between runs (default 5s)
    
    Returns:
        Response database with all results
    """
    
    responses = []
    call_count = 0
    session_start = datetime.now()
    
    # Sequential fragment processing
    for fragment in fragments:
        print(f"\nProcessing {fragment['id']}...")
        
        # All models evaluate fragment before moving to next
        for model in models:
            for prompt_type in prompts:
                for run_num in range(1, runs_per_combo + 1):
                    
                    # Make API call (see G.2 for retry logic)
                    response = make_api_call(
                        fragment=fragment,
                        model=model,
                        prompt_type=prompt_type,
                        run_number=run_num
                    )
                    
                    # Store response immediately
                    responses.append(response)
                    call_count += 1
                    
                    # Rate limiting
                    if run_num < runs_per_combo:
                        time.sleep(rate_limit_delay)
                    
                    # Batch checkpoint
                    if call_count >= batch_size:
                        save_checkpoint(responses, session_start)
                        print(f"Checkpoint: {call_count} calls completed")
    
    return responses

def save_checkpoint(responses, session_start):
    """Save current batch to prevent data loss."""
    df = pd.DataFrame(responses)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    df.to_csv(f'responses_checkpoint_{timestamp}.csv', index=False)

Session Management
python
def calculate_session_plan(total_fragments=150, models=6, prompts=2, 
                          runs=3, calls_per_session=1000):
    """
    Calculate execution plan across multiple sessions.
    
    Total calls = fragments × models × prompts × runs
                = 150 × 6 × 2 × 3 = 5,400
    
    Calls per fragment = models × prompts × runs = 36
    Fragments per session = calls_per_session ÷ 36 ≈ 27-28 fragments
    """
    
    total_calls = total_fragments * models * prompts * runs
    calls_per_fragment = models * prompts * runs
    fragments_per_session = calls_per_session // calls_per_fragment
    total_sessions = (total_fragments + fragments_per_session - 1) // fragments_per_session
    
    plan = {
        'total_api_calls': total_calls,
        'calls_per_fragment': calls_per_fragment,
        'fragments_per_session': fragments_per_session,
        'estimated_sessions': total_sessions,
        'calls_per_session': calls_per_session
    }
    
    print(f"Execution Plan:")
    print(f"  Total API calls: {plan['total_api_calls']:,}")
    print(f"  Fragments per session: {plan['fragments_per_session']}")
    print(f"  Estimated sessions: {plan['estimated_sessions']}")
    print(f"  Approximate calls per session: {plan['calls_per_session']}")
    
    return plan

# Example usage
session_plan = calculate_session_plan()
# Output:
#   Total API calls: 5,400
#   Fragments per session: 27
#   Estimated sessions: 6
#   Approximate calls per session: 972
Execution Progress Tracker
python
def track_execution_progress(responses, total_expected=5400):
    """Monitor completion status during execution."""
    
    df = pd.DataFrame(responses)
    
    # Overall progress
    completed = len(df[df['status'] == 'success'])
    failed = len(df[df['status'] == 'failed'])
    
    # Fragment-level completion
    fragments_complete = df.groupby('fragment_id').apply(
        lambda x: len(x) == 36  # 6 models × 2 prompts × 3 runs
    ).sum()
    
    progress = {
        'completed_calls': completed,
        'failed_calls': failed,
        'total_calls': len(df),
        'completion_rate': completed / total_expected * 100,
        'fragments_complete': fragments_complete,
        'fragments_remaining': 150 - fragments_complete
    }
    
    print(f"\nProgress Update:")
    print(f"  Completed: {progress['completed_calls']:,} / {total_expected:,} ({progress['completion_rate']:.1f}%)")
    print(f"  Failed: {progress['failed_calls']}")
    print(f"  Fragments complete: {progress['fragments_complete']} / 150")
    
    return progress


 
Appendix G.2: Error Handling and Retry Logic
Error Type Detection and Categorization
python
class APIError:
    """Categorize API errors for appropriate handling."""
    
    # Error categories
    TIMEOUT = "timeout"
    RATE_LIMIT = "rate_limit_exceeded"
    SERVICE_UNAVAILABLE = "service_unavailable"
    INVALID_RESPONSE = "invalid_response"
    SAFETY_TRIGGER = "safety_policy_triggered"
    API_ERROR = "api_error"
    OTHER = "other"
    
    @staticmethod
    def categorize_error(error, response_text=None):
        """
        Categorize error based on exception type and response content.
        
        Args:
            error: Exception object from API call
            response_text: Response body if available
            
        Returns:
            Tuple of (error_category, error_message)
        """
        error_str = str(error).lower()
        
        # Timeout errors
        if "timeout" in error_str or "timed out" in error_str:
            return (APIError.TIMEOUT, str(error))
        
        # Rate limiting
        if "rate limit" in error_str or "429" in error_str:
            return (APIError.RATE_LIMIT, str(error))
        
        # Service unavailable
        if "503" in error_str or "502" in error_str or "unavailable" in error_str:
            return (APIError.SERVICE_UNAVAILABLE, str(error))
        
        # Safety policy triggers
        if response_text and any(phrase in response_text.lower() for phrase in 
                                ["content policy", "safety", "harmful content"]):
            return (APIError.SAFETY_TRIGGER, "Safety policy triggered")
        
        # Invalid/malformed response
        if "json" in error_str or "parse" in error_str or "format" in error_str:
            return (APIError.INVALID_RESPONSE, str(error))
        
        # Generic API error
        if "api" in error_str or "400" in error_str or "401" in error_str:
            return (APIError.API_ERROR, str(error))
        
        # Unknown error type
        return (APIError.OTHER, str(error))


Exponential Backoff Implementation
python
import time

def exponential_backoff(attempt_number, base_delay=10):
    """
    Calculate wait time using exponential backoff.
    
    Attempt 1: 10 seconds
    Attempt 2: 30 seconds  
    Attempt 3: 90 seconds
    
    Args:
        attempt_number: Current retry attempt (1-3)
        base_delay: Base delay in seconds (default 10)
        
    Returns:
        Wait time in seconds
    """
    
    delays = {
        1: base_delay,           # 10 seconds
        2: base_delay * 3,       # 30 seconds
        3: base_delay * 9        # 90 seconds
    }
    
    return delays.get(attempt_number, base_delay * 9)


def wait_with_progress(seconds, message="Waiting"):
    """Display countdown during wait period."""
    print(f"{message} {seconds}s...", end="", flush=True)
    time.sleep(seconds)
    print(" Done.")


Retry Decision Logic
python
def should_retry(error_category, attempt_number, max_attempts=3):
    """
    Determine if API call should be retried based on error type.
    
    Args:
        error_category: Error category from APIError.categorize_error()
        attempt_number: Current attempt number (1-based)
        max_attempts: Maximum retry attempts (default 3)
        
    Returns:
        Boolean indicating whether to retry
    """
    
    # Don't retry if max attempts reached
    if attempt_number >= max_attempts:
        return False
    
    # Retry for transient errors
    retriable_errors = [
        APIError.TIMEOUT,
        APIError.RATE_LIMIT,
        APIError.SERVICE_UNAVAILABLE
    ]
    
    # Don't retry for permanent errors
    permanent_errors = [
        APIError.SAFETY_TRIGGER,
        APIError.API_ERROR,
        APIError.INVALID_RESPONSE
    ]
    
    if error_category in retriable_errors:
        return True
    elif error_category in permanent_errors:
        return False
    else:
        # For unknown errors, retry once
        return attempt_number == 1


Main API Call Wrapper with Retry Logic
python
def make_api_call_with_retry(fragment, model, prompt_type, run_number, 
                             max_attempts=3):
    """
    Execute API call with automatic retry logic.
    
    Args:
        fragment: Fragment dictionary with 'id' and 'text'
        model: Model identifier from API_CONFIG
        prompt_type: 'zero_shot' or 'few_shot'
        run_number: Run number (1-3)
        max_attempts: Maximum retry attempts (default 3)
        
    Returns:
        Response dictionary with status and data/error info
    """
    
    attempt = 0
    last_error = None
    
    while attempt < max_attempts:
        attempt += 1
        
        try:
            # Make the actual API call (implementation in G.3)
            response = execute_api_request(fragment, model, prompt_type, run_number)
            
            # Success - return immediately
            return {
                'fragment_id': fragment['id'],
                'model': model,
                'prompt_type': prompt_type,
                'run_number': run_number,
                'status': 'success',
                'response': response,
                'attempts': attempt,
                'timestamp': datetime.now().isoformat(),
                'error': None
            }
            
        except Exception as e:
            # Categorize the error
            error_category, error_message = APIError.categorize_error(e)
            last_error = (error_category, error_message)
            
            # Log attempt
            print(f"  Attempt {attempt} failed: {error_category}")
            
            # Decide whether to retry
            if should_retry(error_category, attempt, max_attempts):
                # Calculate wait time
                wait_time = exponential_backoff(attempt)
                
                # Wait before retry
                wait_with_progress(
                    wait_time, 
                    f"  Retry in"
                )
            else:
                # Don't retry - break loop
                break
    
    # All retries exhausted or permanent error
    error_category, error_message = last_error
    
    return {
        'fragment_id': fragment['id'],
        'model': model,
        'prompt_type': prompt_type,
        'run_number': run_number,
        'status': 'failed',
        'response': None,
        'attempts': attempt,
        'timestamp': datetime.now().isoformat(),
        'error': {
            'category': error_category,
            'message': error_message
        }
    }


Failed Call Logging
python
import json

def log_failed_call(response_dict, log_file='failed_calls.jsonl'):
    """
    Log failed API calls for manual review and rerun.
    
    Args:
        response_dict: Response dictionary from make_api_call_with_retry
        log_file: Path to failed calls log (JSON Lines format)
    """
    
    if response_dict['status'] != 'failed':
        return  # Only log failures
    
    # Extract relevant information
    failed_call = {
        'fragment_id': response_dict['fragment_id'],
        'model': response_dict['model'],
        'prompt_type': response_dict['prompt_type'],
        'run_number': response_dict['run_number'],
        'error_category': response_dict['error']['category'],
        'error_message': response_dict['error']['message'],
        'attempts': response_dict['attempts'],
        'timestamp': response_dict['timestamp']
    }
    
    # Append to log file (JSON Lines format - one JSON object per line)
    with open(log_file, 'a') as f:
        f.write(json.dumps(failed_call) + '\n')
    
    print(f"  Logged failed call: {response_dict['fragment_id']} / {response_dict['model']}")


Rerun Queue Management
python
def load_failed_calls(log_file='failed_calls.jsonl'):
    """
    Load failed calls from log for rerun session.
    
    Returns:
        List of failed call dictionaries
    """
    
    failed_calls = []
    
    try:
        with open(log_file, 'r') as f:
            for line in f:
                failed_calls.append(json.loads(line))
    except FileNotFoundError:
        print(f"No failed calls log found at {log_file}")
        return []
    
    print(f"Loaded {len(failed_calls)} failed calls for rerun")
    return failed_calls


def rerun_failed_calls(failed_calls, fragment_data):
    """
    Rerun failed API calls in a separate session.
    
    Args:
        failed_calls: List of failed call dictionaries from load_failed_calls()
        fragment_data: Dictionary mapping fragment_id to fragment content
        
    Returns:
        List of response dictionaries
    """
    
    print(f"\n{'='*60}")
    print(f"RERUN SESSION - {len(failed_calls)} calls to retry")
    print(f"{'='*60}\n")
    
    rerun_responses = []
    
    for i, failed_call in enumerate(failed_calls, 1):
        print(f"[{i}/{len(failed_calls)}] Retrying {failed_call['fragment_id']} / {failed_call['model']}...")
        
        # Get fragment data
        fragment = fragment_data.get(failed_call['fragment_id'])
        if not fragment:
            print(f"  ERROR: Fragment {failed_call['fragment_id']} not found")
            continue
        
        # Retry the call
        response = make_api_call_with_retry(
            fragment=fragment,
            model=failed_call['model'],
            prompt_type=failed_call['prompt_type'],
            run_number=failed_call['run_number']
        )
        
        rerun_responses.append(response)
        
        # Brief pause between reruns
        if i < len(failed_calls):
            time.sleep(5)
    
    # Summary
    successful = sum(1 for r in rerun_responses if r['status'] == 'success')
    still_failed = len(rerun_responses) - successful
    
    print(f"\n{'='*60}")
    print(f"RERUN SUMMARY:")
    print(f"  Successful: {successful}/{len(rerun_responses)}")
    print(f"  Still failed: {still_failed}/{len(rerun_responses)}")
    print(f"{'='*60}\n")
    
    return rerun_responses


def clear_failed_calls_log(log_file='failed_calls.jsonl'):
    """Clear failed calls log after successful rerun."""
    import os
    if os.path.exists(log_file):
        os.remove(log_file)
        print(f"Cleared failed calls log: {log_file}")


 
Appendix G.3: Response Capture Parsing
Response Database Schema
python
import pandas as pd
from datetime import datetime
import os

# Supported parameters per API family
# Maps universal param names → provider-specific names
# None value means parameter is not supported
PARAM_MAPPING = {
    'openai_compatible': {
        # GPT, DeepSeek, Kimi, GLM all use OpenAI-compatible format
        'temperature': 'temperature',
        'max_tokens': 'max_tokens',
        'top_p': 'top_p',
        'frequency_penalty': 'frequency_penalty',
        'presence_penalty': 'presence_penalty'
    },
    'anthropic': {
        'temperature': 'temperature',
        'max_tokens': 'max_tokens',
        'top_p': 'top_p',
        'frequency_penalty': None,  # Not supported
        'presence_penalty': None    # Not supported
    },
    'google': {
        'temperature': 'temperature',
        'max_tokens': 'maxOutputTokens',
        'top_p': 'topP',
        'frequency_penalty': None,  # Not supported
        'presence_penalty': None    # Not supported
    }
}

# Map model families to API families
MODEL_API_FAMILY = {
    'gpt_5': 'openai_compatible',
    'claude_opus_4_6': 'anthropic',
    'gemini_3_pro': 'google',
    'deepseek_v3': 'openai_compatible',
    'kimi_k2': 'openai_compatible',
    'glm_4_7': 'openai_compatible'
}


def filter_params(model, universal_params):
    """
    Filter universal parameters to only those supported by the target model.
    
    Args:
        model: Model identifier from API_CONFIG
        universal_params: Dictionary of universal parameter names and values
    
    Returns:
        Dictionary with provider-specific parameter names and values,
        excluding unsupported parameters
    """
    api_family = MODEL_API_FAMILY.get(model, 'openai_compatible')
    mapping = PARAM_MAPPING[api_family]
    
    filtered = {}
    for universal_name, value in universal_params.items():
        provider_name = mapping.get(universal_name)
        if provider_name is not None:
            filtered[provider_name] = value
    
    return filtered


def build_request_headers(config):
    """Construct authentication headers for API request."""
    api_key = os.getenv(config['api_key_env'])
    
    if not api_key:
        raise ValueError(
            f"API key not found in environment variable: {config['api_key_env']}"
        )
    
    if config['auth_type'] == 'bearer':
        return {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
    elif config['auth_type'] == 'x-api-key':
        return {
            'x-api-key': api_key,
            'Content-Type': 'application/json',
            'anthropic-version': '2024-10-22'  # Update at execution time
        }
    elif config['auth_type'] == 'api_key_param':
        return {'Content-Type': 'application/json'}
    else:
        raise ValueError(f"Unknown auth_type: {config['auth_type']}")


def build_request_payload(config, prompt, params, model):
    """
    Construct request payload with model-specific parameter handling.
    
    Args:
        config: Model configuration dictionary from API_CONFIG
        prompt: Formatted prompt string
        params: STANDARD_PARAMS dictionary
        model: Model identifier (for parameter filtering)
    
    Returns:
        Dictionary ready for json= argument in requests.post()
    """
    # Filter parameters for this model's API
    filtered_params = filter_params(model, params)
    
    api_family = MODEL_API_FAMILY.get(model, 'openai_compatible')
    
    if api_family == 'anthropic':
        payload = {
            'model': config['model_id'],
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': filtered_params.pop('max_tokens', 500),
        }
        # Add remaining supported params
        payload.update(filtered_params)
    
    elif api_family == 'google':
        payload = {
            'contents': [
                {'parts': [{'text': prompt}]}
            ],
            'generationConfig': filtered_params
        }
        # Handle API key as URL parameter for Gemini
        # Note: endpoint URL needs ?key=API_KEY appended at call time
    
    else:  # openai_compatible
        payload = {
            'model': config['model_id'],
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
        }
        payload.update(filtered_params)
    
    return payload


def build_endpoint_url(config, model):
    """
    Construct the full endpoint URL, handling Gemini's API key parameter.
    
    Args:
        config: Model configuration dictionary
        model: Model identifier
    
    Returns:
        Full endpoint URL string
    """
    endpoint = config['endpoint']
    
    if config['auth_type'] == 'api_key_param':
        # Gemini uses API key as URL parameter
        api_key = os.getenv(config['api_key_env'])
        endpoint = f"{endpoint}?key={api_key}"
    
    return endpoint


def execute_api_request(fragment, model, prompt_type, run_number):
    """
    Execute single API request to specified model.
    
    Args:
        fragment: Fragment dictionary with 'id' and 'text'
        model: Model identifier from API_CONFIG
        prompt_type: 'zero_shot' or 'few_shot'
        run_number: Run number (1-3)
    
    Returns:
        Parsed response dictionary
    
    Raises:
        Exception for any API errors (handled by retry logic in G.2)
    """
    import requests
    import time
    
    # Get model configuration
    config = API_CONFIG[model]
    
    # Load prompt template
    prompt = load_prompt_template(prompt_type, fragment['text'])
    
    # Construct request components
    headers = build_request_headers(config)
    payload = build_request_payload(config, prompt, STANDARD_PARAMS, model)
    endpoint = build_endpoint_url(config, model)
    
    # Execute request with timing
    start_time = time.time()
    response = requests.post(
        endpoint,
        headers=headers,
        json=payload,
        timeout=60
    )
    end_time = time.time()
    latency = end_time - start_time
    
    # Check for HTTP errors
    response.raise_for_status()
    
    # Parse response
    parsed = parse_api_response(response.json(), model)
    
    # Add metadata
    parsed['latency_seconds'] = latency
    parsed['api_version'] = config['model_id']
    
    return parsed


def load_prompt_template(prompt_type, fragment_text):
    """Load prompt template and insert fragment text."""
    
    if prompt_type == 'zero_shot':
        template_path = 'prompts/zero_shot_template.txt'
    else:  # few_shot
        template_path = 'prompts/few_shot_template.txt'
    
    with open(template_path, 'r') as f:
        template = f.read()
    
    # Insert fragment text into template
    prompt = template.replace('[Fragment text will be inserted here]', fragment_text)
    
    return prompt


def build_request_headers(config):
    """Construct authentication headers for API request."""
    
    api_key = os.getenv(config['api_key_env'])
    
    if config['auth_type'] == 'bearer':
        return {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
    elif config['auth_type'] == 'x-api-key':
        return {
            'x-api-key': api_key,
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
    elif config['auth_type'] == 'api_key_param':
        return {'Content-Type': 'application/json'}
    
    
def build_request_payload(config, prompt, params):
    """Construct request payload based on model API format."""
    
    # Most models use OpenAI-compatible format
    base_payload = {
        'model': config['model_id'],
        'messages': [
            {'role': 'user', 'content': prompt}
        ],
        'temperature': params['temperature'],
        'max_tokens': params['max_tokens']
    }
    
    # Model-specific adjustments
    if 'claude' in config['model_id']:
        # Claude uses different format
        base_payload = {
            'model': config['model_id'],
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'max_tokens': params['max_tokens'],
            'temperature': params['temperature']
        }
    
    if 'gemini' in config['model_id']:
        # Gemini uses different format
        base_payload = {
            'contents': [
                {'parts': [{'text': prompt}]}
            ],
            'generationConfig': {
                'temperature': params['temperature'],
                'maxOutputTokens': params['max_tokens']
            }
        }
    
    return base_payload

Response Parsing
python
import json
import re

def parse_api_response(response_json, model):
    """
    Parse API response into standardized format.
    Handles both JSON structured outputs and text parsing.
    
    Args:
        response_json: Raw JSON response from API
        model: Model identifier for format detection
        
    Returns:
        Dictionary with parsed classification, rationale, and token counts
    """
    
    # Extract text content based on model format
    content = extract_response_content(response_json, model)
    
    # Try JSON parsing first (for models with structured output)
    try:
        parsed = parse_structured_json(content)
        return {
            'classification': parsed['classification'],
            'rationale': parsed['rationale'],
            'token_count_input': get_input_tokens(response_json, model),
            'token_count_output': get_output_tokens(response_json, model),
            'parse_method': 'json'
        }
    except:
        pass  # Fall back to text parsing
    
    # Text parsing for unstructured responses
    parsed = parse_unstructured_text(content)
    
    return {
        'classification': parsed['classification'],
        'rationale': parsed['rationale'],
        'token_count_input': get_input_tokens(response_json, model),
        'token_count_output': get_output_tokens(response_json, model),
        'parse_method': 'text'
    }


def extract_response_content(response_json, model):
    """Extract text content from response based on model format."""
    
    # OpenAI-compatible format (GPT, DeepSeek, Kimi, GLM)
    if 'choices' in response_json:
        return response_json['choices'][0]['message']['content']
    
    # Claude format
    elif 'content' in response_json:
        return response_json['content'][0]['text']
    
    # Gemini format
    elif 'candidates' in response_json:
        return response_json['candidates'][0]['content']['parts'][0]['text']
    
    else:
        raise ValueError(f"Unknown response format for model: {model}")


def parse_structured_json(content):
    """
    Parse JSON-formatted response.
    Expected format:
    {
        "classification": "sound" or "not sound",
        "rationale": "2-4 sentence explanation"
    }
    """
    
    # Handle code blocks around JSON
    content = content.strip()
    if content.startswith('```'):
        # Remove markdown code fences
        content = re.sub(r'^```json?\n', '', content)
        content = re.sub(r'\n```$', '', content)
    
    parsed = json.loads(content)
    
    return {
        'classification': parsed.get('classification', '').strip().lower(),
        'rationale': parsed.get('rationale', '').strip()
    }


def parse_unstructured_text(content):
    """
    Parse free-text response using pattern matching.
    Expected format:
    Classification: sound
    Rationale: [2-4 sentences]
    """
    
    # Pattern for classification
    classification_pattern = r'(?:classification|judgment):\s*(sound|not sound)'
    classification_match = re.search(classification_pattern, content, re.IGNORECASE)
    
    if classification_match:
        classification = classification_match.group(1).lower()
    else:
        # Try to find standalone "sound" or "not sound"
        if re.search(r'\b(not sound)\b', content, re.IGNORECASE):
            classification = 'not sound'
        elif re.search(r'\bsound\b', content, re.IGNORECASE):
            classification = 'sound'
        else:
            classification = None
    
    # Pattern for rationale
    rationale_pattern = r'(?:rationale|explanation):\s*(.+?)(?:\n\n|\Z)'
    rationale_match = re.search(rationale_pattern, content, re.IGNORECASE | re.DOTALL)
    
    if rationale_match:
        rationale = rationale_match.group(1).strip()
    else:
        # Take everything after classification
        if classification:
            rationale = content.split(classification, 1)[1].strip()
        else:
            rationale = content.strip()
    
    return {
        'classification': classification,
        'rationale': rationale
    }


def get_input_tokens(response_json, model):
    """Extract input token count from response."""
    
    if 'usage' in response_json:
        return response_json['usage'].get('prompt_tokens', 0)
    elif 'usageMetadata' in response_json:  # Gemini
        return response_json['usageMetadata'].get('promptTokenCount', 0)
    else:
        return 0


def get_output_tokens(response_json, model):
    """Extract output token count from response."""
    
    if 'usage' in response_json:
        return response_json['usage'].get('completion_tokens', 0)
    elif 'usageMetadata' in response_json:  # Gemini
        return response_json['usageMetadata'].get('candidatesTokenCount', 0)
    else:
        return 0

Classification Normalization
python
def normalize_classification(raw_classification):
    """
    Normalize variant phrasings to standard format.
    
    Args:
        raw_classification: Raw classification string from model
        
    Returns:
        'sound', 'not_sound', or None if invalid
    """
    
    if not raw_classification:
        return None
    
    normalized = raw_classification.strip().lower()
    
    # Sound variants
    sound_variants = [
        'sound',
        'yes',
        'valid',
        'acceptable',
        'defensible',
        'pass'
    ]
    
    # Not sound variants
    not_sound_variants = [
        'not sound',
        'unsound',
        'no',
        'invalid',
        'unacceptable',
        'indefensible',
        'fail',
        'weak'
    ]
    
    # Check for matches
    if any(variant in normalized for variant in not_sound_variants):
        return 'not_sound'
    elif any(variant == normalized for variant in sound_variants):
        return 'sound'
    else:
        # Ambiguous - flag for manual review
        print(f"  WARNING: Ambiguous classification '{raw_classification}'")
        return None

Rationale Validation
python
def validate_rationale(rationale_text):
    """
    Validate that rationale meets quality criteria.
    
    Criteria:
    1. Minimum 10 words
    2. Contains substantive reasoning (not just restatement)
    
    Args:
        rationale_text: Rationale text from model
        
    Returns:
        Tuple of (is_valid, issue_description)
    """
    
    if not rationale_text:
        return (False, "Rationale is empty")
    
    # Check word count
    words = rationale_text.split()
    if len(words) < 10:
        return (False, f"Rationale too short ({len(words)} words, minimum 10)")
    
    # Check for substantive reasoning (not just restatement of classification)
    # Simple heuristic: should contain domain-related keywords
    reasoning_keywords = [
        'evidence', 'criteria', 'standard', 'warrant', 'synthesis',
        'integration', 'conclusion', 'judgment', 'evaluation',
        'reasoning', 'argument', 'basis', 'support', 'limitation',
        'qualification', 'context', 'demonstrates', 'lacks', 'shows'
    ]
    
    text_lower = rationale_text.lower()
    keyword_count = sum(1 for keyword in reasoning_keywords if keyword in text_lower)
    
    if keyword_count < 2:
        return (False, "Rationale lacks substantive reasoning (no domain keywords)")
    
    return (True, None)

Complete Response Processing
python
def process_and_store_response(fragment, model, prompt_type, run_number, 
                               parsed_response, database_df):
    """
    Process parsed response, normalize, validate, and add to database.
    
    Args:
        fragment: Fragment dictionary
        model: Model identifier
        prompt_type: 'zero_shot' or 'few_shot'
        run_number: Run number (1-3)
        parsed_response: Parsed response from execute_api_request
        database_df: Response database DataFrame
        
    Returns:
        Updated database DataFrame
    """
    
    # Normalize classification
    normalized_class = normalize_classification(parsed_response['classification'])
    
    # Validate rationale
    is_valid, validation_issue = validate_rationale(parsed_response['rationale'])
    
    # Determine error flag
    error_flag = (normalized_class is None) or (not is_valid)
    error_details = validation_issue if not is_valid else (
        "Invalid classification" if normalized_class is None else None
    )
    
    # Create response record
    response_record = {
        'fragment_id': fragment['id'],
        'model_family': model,
        'prompt_condition': prompt_type,
        'run_number': run_number,
        'classification_output': normalized_class,
        'rationale_text': parsed_response['rationale'],
        'timestamp': datetime.now().isoformat(),
        'api_latency_seconds': parsed_response['latency_seconds'],
        'token_count_input': parsed_response['token_count_input'],
        'token_count_output': parsed_response['token_count_output'],
        'api_version': parsed_response['api_version'],
        'error_flag': error_flag,
        'error_details': error_details
    }
    
    # Append to database
    database_df = pd.concat([database_df, pd.DataFrame([response_record])], ignore_index=True)
    
    # Save immediately to prevent data loss
    save_response_database(database_df)
    
    return database_df


